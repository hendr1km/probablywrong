[
  {
    "objectID": "posts/weights/logistic_sim.html",
    "href": "posts/weights/logistic_sim.html",
    "title": "Exploring frequency weights in logistic regression",
    "section": "",
    "text": "In logistic regression, we estimate a linear model for the log odds of observing an event at different levels of our predictor. These log odds can be transformed into probabilities, which then produce the characteristic S-curve.\nIn essence, it all comes down to odds. Having odds of 1:1 in our underlying data generation process means that both events are equally likely to occur, each with a probability of 50%. Our fictional population for the simulation follows this setup, with the probabilities of our events depending on values from normal distributions for both groups, with means of 5 and 8 and standard deviations of 1.\nlibrary(tidyverse)\n\n    a &lt;- tibble(value = rnorm(4000, 5, 1), condition = \"a\")\n    b &lt;- tibble(value = rnorm(4000, 8, 1), condition = \"b\")\n    df_ab &lt;- bind_rows(a, b)\n\ndf_ab %&gt;%\n  ggplot(aes(x = value, y = condition, fill = condition))+\n  ggdist::stat_halfeye()+ \n  scale_x_continuous(breaks = seq(1,12,1))+\n  blog_theme()+\n  guides(fill = \"none\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#simulating-imbalance",
    "href": "posts/weights/logistic_sim.html#simulating-imbalance",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Simulating imbalance",
    "text": "Simulating imbalance\nTo get an idea of how frequency weights affect the logistic regression results, we will generate some simulations from our fictitious population under different conditions. Each simulation will consist of 400 samples, with each N = 400 values drawn from our population.\n\nBalanced dataset: Each sample will have a 1:1 ratio of our two classes and represent the proportions in the population.\nUnbalanced dataset: Each sample will have a 1:4 ratio, representing an imbalance between the classes caused by some sort of sampling bias, where where one group is underrepresented in the sample, compared to the population.\nWeighted unbalanced dataset: Similar to the unbalanced dataset, but here each observation in the underrepresented class is given a weight of 4 to counteract the imbalance.\n\nFor each sample under these conditions we will run a logistic regression using the logistic_sim function on each sample.\nWe will then compute the median of the estimated parameters (e.g.Â coefficients and intercepts) from each condition in order to obtain a robust measure of central tendency.\nFinally, we will aggregate these median parameter estimates across all conditions into a single logistic regression model for each condition.\n\nlogistic_sim functionconditions\n\n\n\nlogistic_sim &lt;- function(sample_n, a_n, b_n, a_mean = 5, b_mean = 8, a_std = 1, b_std = 1, weight = 1){\n  map(1:sample_n, ~ {\n    # Create samples for conditions a and b\n    a &lt;- tibble(value = rnorm(a_n, a_mean, a_std), condition = 0)\n    b &lt;- tibble(value = rnorm(b_n, b_mean, b_std), condition = 1)\n    df_ab &lt;- bind_rows(a, b)\n\n    # Adjust weights if the argument is given and not 1 \n    if (weight != 1){\n      df_ab &lt;- df_ab %&gt;%\n        mutate(weights = ifelse(condition == 0, weight, 1))\n    }\n\n    return(df_ab)\n  }) %&gt;%\n  imap(~ {\n    if (weight != 1) {\n      # Fit model with weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x, weights = .x$weights)\n    } else {\n      # Fit model without weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x)\n    }\n    # Tidy and return the model coefficients with iteration number\n    broom::tidy(model) %&gt;%\n      select(term, estimate) %&gt;%\n      mutate(iteration = .y)\n  }) %&gt;%\n  bind_rows() %&gt;%\n  pivot_wider(names_from = term, values_from = estimate) %&gt;%\n  rename(alpha = `(Intercept)`, beta = value)\n}\n\n\n\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nbalanced_estimates &lt;- logistic_sim(400, a_n = 200, b_n = 200) %&gt;%\n  mutate(model = \"balanced\")\nunbalanced_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333) %&gt;%\n  mutate(model = \"unbalanced\")\nweighted_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333, weight = 4) %&gt;%\n  mutate(model = \"weighted\")\n\n\n\n\nAs we can see, the unbalanced model underestimates the first group compared to the balanced model. It seems that the intercept is particularly affected, while the slope is relatively similar. However, all the estimated probabilities are affected.\nAssuming that our balanced model is the closest estimate we can get of our population, the unbalanced model that used weights seems to be better fitted. Assuming that our two events are generally equally likely to occur in our population, but our sample is biased, we might be interested in adjusting these estimates using frequency weights.\n\nbind_rows(balanced_estimates, unbalanced_estimates, weighted_estimates) %&gt;%\ngroup_by(model) %&gt;%\nsummarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  mutate(x = list(seq(3,9,.1))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x)) %&gt;%\n  ggplot(aes(x = x, y = y, col = model))+\n  geom_line()+\n  xlab(\"value\")+\n  ylab(\"probability\")+\n  blog_theme()"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "href": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Relationship between odds and weights",
    "text": "Relationship between odds and weights\nBut how do we determine an appropriate frequency weight?\nTo explore this, we experiment with all possible combinations of odds and weights from 1 to 20 in our fictional population. Again, we use the logistic_sim function to estimate our median parameters under each combination.\n\nsample_n = 400\nodds = 1:20\na_n = round(sample_n / (odds + 1))\nb_n = 400 - a_n\nweight = list(1:20)\n\nsim_models &lt;- tibble(sample_n, odds, a_n, b_n, weight) %&gt;%\n  unnest(weight)\n\npsych::headTail(sim_models)\n\n  sample_n odds a_n b_n weight\n1      400    1 200 200      1\n2      400    1 200 200      2\n3      400    1 200 200      3\n4      400    1 200 200      4\n5      ...  ... ... ...    ...\n6      400   20  19 381     17\n7      400   20  19 381     18\n8      400   20  19 381     19\n9      400   20  19 381     20\n\n\n```{r}\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nmodel_estimates &lt;- sim_models %&gt;%\n  mutate(estimates = pmap(list(sample_n = sample_n, a_n = a_n, b_n = b_n, weight = weight), logistic_sim, .progress = TRUE))\n\n```"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#evaluating-parameter-estimates-for-different-odds-and-weights",
    "href": "posts/weights/logistic_sim.html#evaluating-parameter-estimates-for-different-odds-and-weights",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Evaluating parameter estimates for different odds and weights",
    "text": "Evaluating parameter estimates for different odds and weights\nWe can evaluate the performance of each model by looking at the median error at different values of our variable of interest - the mean of the two groups and the midpoint between these means. As you can see in the graph, at the critical value of 6.5, the median error tends to decrease up to a certain weight threshold, after which it increases again. So even in such a simple example, weights only help to a certain extent and must be used with caution.\n\n\nCode\npred &lt;- model_estimates %&gt;%\n  unnest(estimates) %&gt;%\n  group_by(odds, a_n, b_n, weight) %&gt;%\n  summarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  ungroup() %&gt;%\n  mutate(x = list(c(5, 6.5, 8))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x))\n\nbench &lt;- pred %&gt;%\n  filter(weight == 1, odds == 1) %&gt;%\n  select(x, y_bench = y)\n\npred_bench &lt;- pred %&gt;%\n  left_join(bench, by = join_by(x)) %&gt;%\n  mutate(median_error = abs(y_bench - y)) \n\npred_min &lt;- pred_bench %&gt;%\n  filter(x == 6.5) %&gt;%\n  group_by(odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(odds, weight)\n\npred_bench %&gt;%\n  # mutate(odds = as.character(paste0(\"1:\",odds))) %&gt;%\n  ggplot(aes(x = weight, y = median_error, col = factor(x)))+\n  geom_line()+\n  facet_wrap(~odds)+\n  blog_theme()+\n  geom_vline(data = pred_min, aes(xintercept = weight), size = 1, color = \"gray\", linetype= \"dotted\")+\n  ylab(\"median error\")+\n  labs(col = \"value\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#choosing-weights",
    "href": "posts/weights/logistic_sim.html#choosing-weights",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Choosing weights",
    "text": "Choosing weights\nThe threshold with the most balanced estimates seems to be closely related to the odds ratio. Thus, choosing the assumed imbalance of the odds as the weight may be the most appropriate, especially for smaller imbalances. At least in our simple setup. However, deviations for higher imbalances may also be due to the uncertainty of the estimates as the sample size for the minority group decreases. As the odds increase, almost any weight seems to produce more appropriate estimates than the unbalanced model where we did not adjust for sampling bias.\n\nno_weight_bench &lt;- pred_bench %&gt;%\n  filter(weight == 1) %&gt;%\n  select(odds, x, median_error_nw = median_error)\n\npred_nh &lt;- pred_bench %&gt;%\n  left_join(no_weight_bench, by = join_by(odds, x), relationship = \"many-to-many\") %&gt;%\n  mutate(improvement = ifelse(median_error &lt;= median_error_nw, TRUE, FALSE))\n\npred_min_error &lt;- pred_bench %&gt;%\n  group_by(x, odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(x, odds, weight) %&gt;%\n  mutate(best_weight = weight) %&gt;%\n  ungroup()\n\npred_nh %&gt;%\n  left_join(pred_min_error, by = join_by(odds, weight, x)) %&gt;%\n  ggplot(aes(x = weight, y = odds))+\n  geom_tile(aes(fill = improvement ), alpha = .5)+\n  geom_line(aes(x = odds, y = odds), linetype= \"dashed\")+\n  geom_point(aes(x = best_weight, y = odds))+\n  facet_wrap(~factor(x))+\n  blog_theme()+\n  labs(fill = \"improvement to\\nunbalanced model\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#conclusion",
    "href": "posts/weights/logistic_sim.html#conclusion",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Conclusion",
    "text": "Conclusion\nDepending on the scenario, weights may be helpful to deal with sampling bias and class imbalance that does not reflect the population (at least in a simple setup, like the one we worked with). However, they should be used with prior knowledge of the population to not overadjust the sampling bias. One solution might be to adjust the observed odds by the expected odds ratio in the population, in the group affected by the sampling bias. The natural class imbalance should also be maintained in the logistic regression unless you have good reasons not to."
  },
  {
    "objectID": "posts/network/network.html",
    "href": "posts/network/network.html",
    "title": "Correlation Networks in R",
    "section": "",
    "text": "To create a correlation network, we will use the BFI data from the {psych} package. First we will import all the necessary packages and set the font for our plot. We can import these from our system using extrafont. In the next step we will do a quick clean up of our data to include only the columns we want to visualise.\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(corrr)\nlibrary(extrafont) # font_import() to import your system fonts\n\nfont = \"Inclusive Sans\"\n\ndf &lt;- psych::bfi %&gt;%\n  tibble() %&gt;%\n  select(-c(age, education, gender))\n\nThe correlate() function from the {corrr} package allows us to define a specific correlation. However, we will use the Pearson correlation as the default. Next, we will only keep correlations higher than .2 or lower than -.2. This is something to play around with. Depending on how many variables you have in your data, your plot could get really messy if you include all the possible relationships between your variables.\n\ngraph_data &lt;- df %&gt;% \n    corrr::correlate() %&gt;% \n    stretch() %&gt;% \n    filter(abs(r) &gt; .2)\n\nThe last step is to build our visualisation. The font variable is the one we set above. This diagram allows you to adjust all the settings you are interested in, such as the size or colour of the nodes. These are your points representing your elements). Feel free to adjust these settings.\n\ngraph_data %&gt;%\n    graph_from_data_frame(directed = FALSE) %&gt;%\n   ggraph(layout = \"kk\") +\n    geom_edge_link(aes(color = r, alpha = r), edge_width = 1) +\n    guides(edge_alpha = \"none\") +\n    scale_edge_colour_gradientn(limits = c(-1, 1), colors = c(\"firebrick2\", \"white\", \"dodgerblue2\")) +\n    geom_node_point(color = \"black\", size = 2) +\n    geom_node_text(aes(label = name), family = font, repel = TRUE) +\n    theme_graph(base_family = font, title_size = 10) +\n    theme(\n      plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(family = font),\n        legend.title = element_text(family = font),\n        legend.text = element_text(family = font)\n    )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "probably wrong",
    "section": "",
    "text": "Correlation Networks in R\n\n\n\n\n\n\n\nR\n\n\nVisualisation\n\n\nTutorial\n\n\n\n\nPlotting correlations as networks can give you a good idea of the interconnectivity of your variables. This is a quick tutorial on how to create correlation networks in R.\n\n\n\n\n\n\nJun 12, 2024\n\n\n\n\n\n\n  \n\n\n\n\nExploring frequency weights in logistic regression\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\n\n\nFrequency weights can be used in logistic regression to address class imbalance caused by sampling bias. In this post, we will use simulations to explore how to effectively choose weights and visually understand the benefits of this approach.\n\n\n\n\n\n\nJun 4, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, Iâm Hendrik. Currently in my masterâs studies in primary education, I also work as a Research Assistant in Clinical Child and Adolescent Psychology and Psychotherapy at the University of Wuppertal. This blog is my space to share everything that fascinates me about R and quantitative research methods"
  }
]