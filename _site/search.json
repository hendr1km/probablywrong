[
  {
    "objectID": "posts/weights/logistic_sim.html",
    "href": "posts/weights/logistic_sim.html",
    "title": "Exploring Weights in Logistic Regression",
    "section": "",
    "text": "In Logistic Regression, we estimate a linear model for the log odds of observing an event across different levels of our predictor. These log odds can be transformed into probabilities, which then create the characteristic S-curve.\nAt the core, it all boils down to odds. Having odds of 1:1 in our underlying data generating process means both events are generally equally likely to occur, each with a probability of 50%. Our fictional population for the simulation follows this setup, where the probabilities of our events are dependent on values drawn from normal distributions for both groups, each with a mean of 5 and 8 and a standard deviation of 1.\nlibrary(tidyverse)\n\n    a &lt;- tibble(value = rnorm(4000, 5, 1), condition = \"a\")\n    b &lt;- tibble(value = rnorm(4000, 8, 1), condition = \"b\")\n    df_ab &lt;- bind_rows(a, b)\n\ndf_ab %&gt;%\n  ggplot(aes(x = value, y = condition, fill = condition))+\n  ggdist::stat_halfeye()+ \n  scale_x_continuous(breaks = seq(1,12,1))+\n  blog_theme()+\n  guides(fill = \"none\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#simulating-imbalance",
    "href": "posts/weights/logistic_sim.html#simulating-imbalance",
    "title": "Exploring Weights in Logistic Regression",
    "section": "simulating imbalance",
    "text": "simulating imbalance\nTo get an idea on how weights effect our model in the long term we simulate 400 samples from our population each with N = 400 under different conditions (balanced with 1:1 odds, unbalanced with 1:4 odds and weighted unbalanced with 1:4 odds and a weight of 4) and perform logistic regressions on each sample. This is all done in the logistic_sim function. Afterwards we compute the median of all those estiamted parameters for each condition and summarise the predictions in a single logistic regression.\n\nlogistic_sim functionconditions\n\n\n\nlogistic_sim &lt;- function(sample_n, a_n, b_n, a_mean = 5, b_mean = 8, a_std = 1, b_std = 1, weight = 1){\n  map(1:sample_n, ~ {\n    # Create samples for conditions a and b\n    a &lt;- tibble(value = rnorm(a_n, a_mean, a_std), condition = 0)\n    b &lt;- tibble(value = rnorm(b_n, b_mean, b_std), condition = 1)\n    df_ab &lt;- bind_rows(a, b)\n\n    # Adjust weights if the argument is given and not 1 \n    if (weight != 1){\n      df_ab &lt;- df_ab %&gt;%\n        mutate(weights = ifelse(condition == 0, weight, 1))\n    }\n\n    # Return df_ab whether weights were adjusted or not\n    return(df_ab)\n  }) %&gt;%\n  imap(~ {\n    if (weight != 1) {\n      # Fit model with weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x, weights = .x$weights)\n    } else {\n      # Fit model without weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x)\n    }\n    # Tidy and return the model coefficients with iteration number\n    broom::tidy(model) %&gt;%\n      select(term, estimate) %&gt;%\n      mutate(iteration = .y)\n  }) %&gt;%\n  bind_rows() %&gt;%\n  pivot_wider(names_from = term, values_from = estimate) %&gt;%\n  rename(alpha = `(Intercept)`, beta = value)\n}\n\n\n\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nbalanced_estimates &lt;- logistic_sim(400, a_n = 200, b_n = 200) %&gt;%\n  mutate(model = \"balanced\")\nunbalanced_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333) %&gt;%\n  mutate(model = \"unbalanced\")\nweighted_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333, weight = 4) %&gt;%\n  mutate(model = \"weighted\")\n\n\n\n\nAs you can see the estimates of the unbalanced model are quite off compared to the balanced one. It seems like the intercept is especially affected and the slope seems relatively similar, although all estimated propabilietes are affected anyway. The unbalanced model that we used weights on doesnt seem to much off. If we assume that our two events are equally likely in our in our population, but our sample is biased choosing the right weight might be a solution.\n\nbind_rows(balanced_estimates, unbalanced_estimates, weighted_estimates) %&gt;%\ngroup_by(model) %&gt;%\nsummarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  mutate(x = list(seq(3,9,.1))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x)) %&gt;%\n  ggplot(aes(x = x, y = y, col = model))+\n  geom_line()+\n  blog_theme()+\n  xlab(\"value\")+\n  ylab(\"probability\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "href": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "title": "Exploring Weights in Logistic Regression",
    "section": "relationship between odds and weights",
    "text": "relationship between odds and weights\nBut how do we know which weight to choose?\nTo get an idea we play around with all possible combinations of odds and weights from 1 to 20 with our fictional population and use the logistic_sim function againt to estimate our median parameters.\n\nsample_n = 400\nodds = 1:20\na_n = round(sample_n / (odds + 1))\nb_n = 400 - a_n\nweight = list(1:20)\n\nsim_models &lt;- tibble(sample_n, odds, a_n, b_n, weight) %&gt;%\n  unnest(weight)\n\npsych::headTail(sim_models)\n\n  sample_n odds a_n b_n weight\n1      400    1 200 200      1\n2      400    1 200 200      2\n3      400    1 200 200      3\n4      400    1 200 200      4\n5      ...  ... ... ...    ...\n6      400   20  19 381     17\n7      400   20  19 381     18\n8      400   20  19 381     19\n9      400   20  19 381     20\n\n\n```{r}\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nmodel_estimates &lt;- sim_models %&gt;%\n  mutate(estimates = pmap(list(sample_n = sample_n, a_n = a_n, b_n = b_n, weight = weight), logistic_sim, .progress = TRUE))\n\n```\nHaving our parameter estiamtes for each possible combination of odds and weights allows us to evaluate how good those predicts are using weights for different odds. Our baseline model is in this case our balanced model. We can see that the median error decreases up to a specific weight and then increaes again. In most cases the mimum error is generated by the model with the same or similar weight as the odds.\n\n#| code-fold: true\n\npred &lt;- model_estimates %&gt;%\n  unnest(estimates) %&gt;%\n  group_by(odds, a_n, b_n, weight) %&gt;%\n  summarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  ungroup() %&gt;%\n  mutate(x = list(c(5, 6.5, 8))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x))\n\nbench &lt;- pred %&gt;%\n  filter(weight == 1, odds == 1) %&gt;%\n  select(x, y_bench = y)\n\npred_bench &lt;- pred %&gt;%\n  left_join(bench, by = join_by(x)) %&gt;%\n  mutate(median_error = abs(y_bench - y)) \n\npred_min &lt;- pred_bench %&gt;%\n  filter(x == 6.5) %&gt;%\n  group_by(odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(odds, weight)\n\npred_bench %&gt;%\n  # mutate(odds = as.character(paste0(\"1:\",odds))) %&gt;%\n  ggplot(aes(x = weight, y = median_error, col = factor(x)))+\n  geom_line()+\n  facet_wrap(~odds)+\n  blog_theme()+\n  geom_vline(data = pred_min, aes(xintercept = weight),\n             size = 1, color = \"gray\", linetype= \"dotted\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#choosing-weights",
    "href": "posts/weights/logistic_sim.html#choosing-weights",
    "title": "Exploring Weights in Logistic Regression",
    "section": "choosing weights",
    "text": "choosing weights\nChoosing the assumed unabalance of the odds as the weight seems like a good idea, especially for the the lower imbalances. Although the deviation in the higher imabalances could also be caused by uncertainty of the estimates as sample size for the minority group becomes increasingly small. With increasing odds the precision of choosing seems less and less important in comparison to the unbalanced model. Or said in another way: using any weight creates way more adjusted estimates than the unweighted logistic regression. However we can also see for the lower odds that there is some increasing tolerance for choosing the wrong weight. At odds of two the weight estiamte can already be 2 to 19 odds off, depending of the x estimate, and still produce more precise estimates than the unweighted model.\n\nno_weight_bench &lt;- pred_bench %&gt;%\n  filter(weight == 1) %&gt;%\n  select(odds, x, median_error_nw = median_error)\n\npred_nh &lt;- pred_bench %&gt;%\n  left_join(no_weight_bench, by = join_by(odds, x), relationship = \"many-to-many\") %&gt;%\n  mutate(improvement = ifelse(median_error &lt;= median_error_nw, TRUE, FALSE))\n\npred_min_error &lt;- pred_bench %&gt;%\n  group_by(x, odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(x, odds, weight) %&gt;%\n  mutate(best_weight = weight) %&gt;%\n  ungroup()\n\npred_nh %&gt;%\n  left_join(pred_min_error, by = join_by(odds, weight, x)) %&gt;%\n  ggplot(aes(x = weight, y = odds))+\n  geom_point(aes(x = best_weight, y = odds))+\n  geom_line(aes(x = odds, y = odds), line_type = \"dashed\")+\n  geom_tile(aes(fill = improvement ), alpha = .5)+\n  facet_wrap(~factor(x))+\n  blog_theme()"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#conclusion",
    "href": "posts/weights/logistic_sim.html#conclusion",
    "title": "Exploring Weights in Logistic Regression",
    "section": "conclusion",
    "text": "conclusion\nWeights offer a solution to deal with imbalance in the data that does not represent the odds in the proportion the therfore odds in the population. However weights should be used calibrated and adjusted to prior knowledge of the population to not cause further harm, but produce more adjusted estimates. We should also keep in mind that class imbalance that occurs natural in our population should not be adjusted but threated as a poperty of the data. Comparing different models for your estimated with and without weight adjustment, especially regarding predictive calibration, might be worth trying. Nevertheless our little simulation left out a lot of conditions to consider and when doing your research in the literature, you should look out for domain specific considerations."
  }
]