[
  {
    "objectID": "posts/partial_resdual/partial_residual.html",
    "href": "posts/partial_resdual/partial_residual.html",
    "title": "Introduction to partial residual plots",
    "section": "",
    "text": "What we have missed\nIn a PRP we plot the residuals or error of our Regression model against one or more independant variables. This allows us a couple of things - starting with the first one: discovering non randomness in the residuals. Why is non randomness an issue? When doing a Regression, we assume that we have got some error as jitter around our predictions that does not contain any remaining trend. This error is caused by all that stuff we do not know about our independent variable and can not model. Theory assumes, that if we had all the variables in the iniverse with a causal relationship to our independent variable , we could make predictions without any error. However this is fictional and error is part of our complex reality, that we can only approxiamte using such a naive linear representation of the world. It is only a problem if our error is not random, because then there is some sort of trend or pattern left in the data that we could have modeled with our variables. In practice this can appear if there is some sort of non linear trend in our data or interactions of variables, that we need to include in our model.\nIn the following example we will start by visualising the residuals from our regression using broom and ggplot by plotting those against one of our independent variable used in the model.\nThrough the use of geom_smoothregression lines we can spot remaining trends in our residuals. The blue regression line is our helper tool to spot those. Keep in mind that we want to have a intercept and slope of approximaetly zero - this means there is no trend left. The red line represents the predictions of our model. In the standard broom::augment() output those are scaled to 0 when plotting our residuals.\nIn the following plot you can see that both lines are approximately on the same slope, indicating that we did not spot any remaining trends in the residual that our model did not catch. At least regarding the variable x3.\n\nm0 &lt;- lm(y0 ~ x1 + x2 + x3)\n\nm0 %&gt;%\n  broom::augment() %&gt;% \n  mutate(x3 = dvmisc::quant_groups(x3, 3)) %&gt;%\n  pivot_longer(c(x1,x2)) %&gt;%\n  ggplot(aes(x = value, y = .resid))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  geom_smooth(aes(x = value, y = 0), method = \"lm\", color = \"red\")+\n  facet_grid(vars(name), vars(x3))\n\n\n\n\nIn the second plot our blue regression lines indicate that there is still some pattern left in our variable x2 that is not included in our model. This is caused by a quadatric relationship that still remains in our residualds. This is a sign that we should adjust our model.\n\nm1 &lt;- lm(y_ac ~ x1 + x2 + x3)\n\nm1 %&gt;%\n  broom::augment() %&gt;% \n  ggplot(aes(x = x2, y = .resid))+\n  xlim(c(50,190))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  geom_smooth(aes(x = x2, y = 0), method = \"lm\", color = \"red\")+\n  ylab(\"residuals of y_ac ~ x1 + x2 + x3\")\n\n\n\n\n\n\nSpotting Interactions\nIn the plots above we only looked at one variable of our model. However we do have multiple variables in our model and we might be interested in looking at them all. One way of doing this is pivoting the data into a long format and visualising those in a grid. Now we can spot non linear trends all of our predictor variables. In the following plot we can see that the residuals for x1 and x3 do not contain any unmodelled information, while the residualds for x2 does.\n\nm1 &lt;- lm(y_ac ~ x1 + x2 + x3)\n\nm1 %&gt;%\n  broom::augment() %&gt;% \n  pivot_longer(c(x1, x2, x3)) %&gt;%\n  ggplot(aes(x = value, y = .resid))+\n  xlim(c(50,190))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  geom_smooth(aes(x = value, y = 0), method = \"lm\", color = \"red\")+\n  facet_grid(vars(name))\n\n\n\n\nHowever in this can we only plotted our predictor variables against the residuals of our model and thus our response variable. To spot how our predictor variables relate to each other we might want to plot those against echt other as well. One way of doing this is plotting those as binned into quantiles in our grid. We can do this using the function dvmisc::quant_groups() and sort the values of x3 into 3 bins based on our quantiles.\nThis allows us to spot interactions between our variables x3 and and x1 or x2 that are not in our model. In this plot you can see that the relationship between x1 and y changes at different qunatiles of x3. For lower values of x3 there is a negative relationship between x1 and x3, while the relationship gets more positive with higher values. We can see that our model currently does not account for this interaction.\n\nm1 &lt;- lm(y1 ~ x1 + x2 + x3)\n\nm1 %&gt;%\n  broom::augment() %&gt;% \n  mutate(x3 = dvmisc::quant_groups(x3, 3)) %&gt;%\n  pivot_longer(c(x1,x2)) %&gt;%\n  ggplot(aes(x = value, y = .resid))+\n  xlim(c(50,190))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  geom_smooth(aes(x = value, y = 0), method = \"lm\", color = \"red\")+\n  facet_grid(vars(name), vars(x3))\n\n\n\n\nThis method can also be used to spot three way interactions. If we already included a two way interaction and still see a change in the relationship of one variable there might be a additional interaction. In the following example we plotted x1 in our bins and thus can see how x2 and x3 vary for different quantiles of x1. We can see that there is no real trend left in the residuals when looking at x2 - we already included the interaction for x2 and x1. For x3 we can see that there is some pattern left and this trend in the residuals varys across x1.\n\nset.seed(123)\nx1 &lt;- rnorm(800, 50, 20)\nx2 &lt;- rnorm(800, 80, 40)\nx3 &lt;- rnorm(800, 90, 50)\nerror &lt;- rnorm(800, 0, 500)\ny1 &lt;- 300 - (0.2 * x1) + (0.2 * x2) + (0.2 * x1 * x2* x3) + error\n\nm3 &lt;- lm(y1 ~ x1 * x2 + x3)\n\nsummary(m3)\n\n\nCall:\nlm(formula = y1 ~ x1 * x2 + x3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-131705   -9143     270   10496  134970 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -71175.508   5705.903 -12.474   &lt;2e-16 ***\nx1            -158.603    104.089  -1.524    0.128    \nx2             -55.711     59.968  -0.929    0.353    \nx3             825.948     17.510  47.170   &lt;2e-16 ***\nx1:x2           20.431      1.159  17.631   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24600 on 795 degrees of freedom\nMultiple R-squared:  0.8729,    Adjusted R-squared:  0.8722 \nF-statistic:  1364 on 4 and 795 DF,  p-value: &lt; 2.2e-16\n\nm3 %&gt;%\n  broom::augment() %&gt;%\n  mutate(x1 = dvmisc::quant_groups(x1, 3)) %&gt;%\n  pivot_longer(c(x3, x2)) %&gt;%\n  # mutate(x3 = paste(\"x3\", x3)) %&gt;%\n  # mutate(x2 = dvmisc::quant_groups(x2, 2)) %&gt;%\n  # mutate(x2 = paste(\"x2\", x2)) %&gt;%\n  ggplot(aes(x = value, y = .resid))+\n  # geom_smooth(aes(x = .fitted))+\n  geom_point()+\n  geom_smooth(aes(x = value, y = .resid), method = \"lm\")+\n  geom_smooth(aes(x = value, y = 0), col = \"red\", method = \"lm\")+\n  facet_grid(vars(name), vars(x1))\n\n\n\n\nThis gets more clear when plotting x3 as our bins and both x1 and x2 still vary.\n\nm3 %&gt;%\n  broom::augment() %&gt;%\n  mutate(x3 = dvmisc::quant_groups(x3, 3)) %&gt;%\n  pivot_longer(c(x1, x2)) %&gt;%\n  # mutate(x3 = paste(\"x3\", x3)) %&gt;%\n  # mutate(x2 = dvmisc::quant_groups(x2, 2)) %&gt;%\n  # mutate(x2 = paste(\"x2\", x2)) %&gt;%\n  ggplot(aes(x = value, y = .resid))+\n  # geom_smooth(aes(x = .fitted))+\n  geom_point()+\n  geom_smooth(aes(x = value, y = .resid), method = \"lm\")+\n  geom_smooth(aes(x = value, y = 0), col = \"red\", method = \"lm\")+\n  facet_grid(vars(name), vars(x3))\n\n\n\n\nIf you are working with real data it might be helpful to use less bins and plot two of your three variables binned into quantiles.\n\nm3 %&gt;%\n  broom::augment() %&gt;%\n  mutate(x3 = dvmisc::quant_groups(x3, 2)) %&gt;%\n  mutate(x3 = paste(\"x3\", x3)) %&gt;%\n  mutate(x2 = dvmisc::quant_groups(x2, 2)) %&gt;%\n  mutate(x2 = paste(\"x2\", x2)) %&gt;%\n  ggplot(aes(x = x1, y = .resid))+\n  # geom_smooth(aes(x = .fitted))+\n  geom_point()+\n  geom_smooth(aes(x = x1, y = .resid), method = \"lm\")+\n  geom_smooth(aes(x = x1, y = 0), col = \"red\", method = \"lm\")+\n  facet_grid(vars(x2), vars(x3))\n\n\n\n\nWhile our residuals are currently centered to our model predictions, but can also plot the model adding back the fitted values from the augment ouput back to residuals. This allows us to plot our predicted slopes for the two way interaction the thus the effects of the third variables on that slopes. In other words we can interpret how our two way interaction varies across our third variable e.g.Â x1 and x2 vary across x3.\n\nm3 %&gt;%\n  broom::augment() %&gt;%\n  mutate(.resid = .fitted + .resid) %&gt;%\n  mutate(x3 = dvmisc::quant_groups(x3, 2)) %&gt;%\n  mutate(x3 = paste(\"x3\", x3)) %&gt;%\n  mutate(x2 = dvmisc::quant_groups(x2, 2)) %&gt;%\n  mutate(x2 = paste(\"x2\", x2)) %&gt;%\n  ggplot(aes(x = x1, y = .resid))+\n  # geom_smooth(aes(x = .fitted))+\n  geom_point()+\n  geom_smooth(aes(x = x1, y = .resid), method = \"lm\")+\n  geom_smooth(aes(x = x1, y = .fitted), col = \"red\", method = \"lm\")+\n  facet_grid(vars(x2), vars(x3))\n\n\n\n\n\n\nBeyond reversed engeneering\nIn all of our plots so far we displayed the residuals of our model against a variables that is itself already a independent variable in the model. However we do not need to. This allows us plot a variable against our response variable while controlling for another variable.\nFor example we might be interested in including x1 into the model. Thus we only want plot x1 against y, while controlling for x2. In comparison to added variable plots, subtracting out the predictor variable and fitting a visual regression to those residuals is not only a approximation of the regression slope, but rather a accurate estimation. By using the same methods we used above, e.g.Â binning the other variable x2 we can also spot interactions before even fitting x1 into the model.\n\ny_av &lt;- 300 - (0.2 * x1) + (0.2 * x3) + (0.2 * x2) + (0.2 * x1 * x2) + error\n\nm4 &lt;- lm(y_av ~ x1 + x2 + x3)\n\nm4 %&gt;%\n  broom::augment() %&gt;% \n  mutate(.resid = .resid + m0$coef[[\"x1\"]] * x1) %&gt;%\n  mutate(x2 = dvmisc::quant_groups(x2, 3)) %&gt;%\n  ggplot(aes(x = x1, y = .resid))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  facet_wrap(vars(x2))\n\n\n\n\n\n\nAdditional notes\nThat were a lot of scatter plots. Keep in mind that it can be helpful to scale and center your variables for plotting.\n\n\nLiterature\nFife, D. (2021, May 17). Visual Partitioning for Multivariate Models: An approach for identifying and visualizing complex multivariate dataset. https://doi.org/10.31234/osf.io/avu2n"
  },
  {
    "objectID": "posts/partial_resdual/partial_residual.html#section",
    "href": "posts/partial_resdual/partial_residual.html#section",
    "title": "Introduction to partial residual plots",
    "section": "",
    "text": "But we are not limited to plotting one variable of our model. As you might have seen our model contains two variables and therefore we can also plot those in panels at onece.\n\nm0 &lt;- lm(y0 ~ x1 + x2)\n\nm0 %&gt;%\n  broom::augment() %&gt;% \n  mutate(x3 = dvmisc::quant_groups(x3, 3)) %&gt;%\n  pivot_longer(c(x1,x2)) %&gt;%\n  ggplot(aes(x = value, y = .resid))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  facet_grid(vars(name), vars(x3))"
  },
  {
    "objectID": "posts/partial_resdual/partial_residual.html#adding-variables",
    "href": "posts/partial_resdual/partial_residual.html#adding-variables",
    "title": "Introduction to partial residual plots",
    "section": "Adding variables",
    "text": "Adding variables\nAnother great thing about PRPs is that we can subtract out variables from our residuals. In our plots above we plotted our predcitors against the full model residuals. In our example we might be interested in including x1 into the model. Thus we only want plot x1 agains the the residuals of x2. Maybe you are already familiar with added variable plots, where we calcualte a seperate regression for y on x2 and then plot those residuals against x1. In a Regression Model the caclulation is however not done stepwise and thus the results that we obtain of this estimation is just an approximation. PRP allow us in comparison to added variable plots to do a accurate estiamation by subtracting out the variable from the model by multiplaying the coefficiant to each value of x1. This allows us to plot a specific variable of our regression model, while adjusting / controlling for others. This is especially helpful to find non linear trends and interaction effects that we need to include into our model.\n\nm0 &lt;- lm(y0 ~ x1 + x2)\n\nm0 %&gt;%\n  broom::augment() %&gt;% \n  mutate(.resid = .resid + m0$coef[[\"x1\"]] * x1) %&gt;%\n  mutate(x3 = dvmisc::quant_groups(x3, 3)) %&gt;%\n  ggplot(aes(x = x1, y = .resid))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  facet_wrap(~x3)"
  },
  {
    "objectID": "posts/partial_resdual/partial_residual.html#three-way-interactions",
    "href": "posts/partial_resdual/partial_residual.html#three-way-interactions",
    "title": "Introduction to partial residual plots",
    "section": "Three way interactions",
    "text": "Three way interactions\n\nset.seed(123)\nx1 &lt;- rnorm(800, 50, 20)\nx2 &lt;- rnorm(800, 80, 40)\nx3 &lt;- rnorm(800, 90, 50)\nerror &lt;- rnorm(800, 0, 500)\ny1 &lt;- 300 - (0.2 * x1) + (0.2 * x2) + (0.2 * x1 * x2* x3) + error\n\nm3 &lt;- lm(y1 ~ x1 * x2 + x3)\n\nsummary(m3)\n\n\nCall:\nlm(formula = y1 ~ x1 * x2 + x3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-131705   -9143     270   10496  134970 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -71175.508   5705.903 -12.474   &lt;2e-16 ***\nx1            -158.603    104.089  -1.524    0.128    \nx2             -55.711     59.968  -0.929    0.353    \nx3             825.948     17.510  47.170   &lt;2e-16 ***\nx1:x2           20.431      1.159  17.631   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24600 on 795 degrees of freedom\nMultiple R-squared:  0.8729,    Adjusted R-squared:  0.8722 \nF-statistic:  1364 on 4 and 795 DF,  p-value: &lt; 2.2e-16\n\nm3 %&gt;%\n  broom::augment() %&gt;%\n  mutate(.resid = .fitted + .resid) %&gt;%\n  mutate(x3 = dvmisc::quant_groups(x3, 2)) %&gt;%\n  mutate(x3 = paste(\"x3\", x3)) %&gt;%\n  mutate(x2 = dvmisc::quant_groups(x2, 2)) %&gt;%\n  mutate(x2 = paste(\"x2\", x2)) %&gt;%\n  ggplot(aes(x = x1, y = .resid))+\n  # geom_smooth(aes(x = .fitted))+\n  geom_point()+\n  geom_smooth(aes(x = x1, y = .resid), method = \"lm\")+\n  geom_smooth(aes(x = x1, y = .fitted), col = \"red\", method = \"lm\")+\n  facet_grid(vars(x2), vars(x3))"
  },
  {
    "objectID": "posts/weights/logistic_sim.html",
    "href": "posts/weights/logistic_sim.html",
    "title": "Exploring frequency weights in logistic regression",
    "section": "",
    "text": "In logistic regression, we estimate a linear model for the log odds of observing an event at different levels of our predictor. These log odds can be transformed into probabilities, which then produce the characteristic S-curve.\nIn essence, it all comes down to odds. Having odds of 1:1 in our underlying data generation process means that both events are equally likely to occur, each with a probability of 50%. Our fictional population for the simulation follows this setup, with the probabilities of our events depending on values from normal distributions for both groups, with means of 5 and 8 and standard deviations of 1.\nlibrary(tidyverse)\n\n    a &lt;- tibble(value = rnorm(4000, 5, 1), condition = \"a\")\n    b &lt;- tibble(value = rnorm(4000, 8, 1), condition = \"b\")\n    df_ab &lt;- bind_rows(a, b)\n\ndf_ab %&gt;%\n  ggplot(aes(x = value, y = condition, fill = condition))+\n  ggdist::stat_halfeye()+ \n  scale_x_continuous(breaks = seq(1,12,1))+\n  blog_theme()+\n  guides(fill = \"none\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#simulating-imbalance",
    "href": "posts/weights/logistic_sim.html#simulating-imbalance",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Simulating imbalance",
    "text": "Simulating imbalance\nTo get an idea of how frequency weights affect the logistic regression results, we will generate some simulations from our fictitious population under different conditions. Each simulation will consist of 400 samples, with each N = 400 values drawn from our population.\n\nBalanced dataset: Each sample will have a 1:1 ratio of our two classes and represent the proportions in the population.\nUnbalanced dataset: Each sample will have a 1:4 ratio, representing an imbalance between the classes caused by some sort of sampling bias, where where one group is underrepresented in the sample, compared to the population.\nWeighted unbalanced dataset: Similar to the unbalanced dataset, but here each observation in the underrepresented class is given a weight of 4 to counteract the imbalance.\n\nFor each sample under these conditions we will run a logistic regression using the logistic_sim function on each sample.\nWe will then compute the median of the estimated parameters (e.g.Â coefficients and intercepts) from each condition in order to obtain a robust measure of central tendency.\nFinally, we will aggregate these median parameter estimates across all conditions into a single logistic regression model for each condition.\n\nlogistic_sim functionconditions\n\n\n\nlogistic_sim &lt;- function(sample_n, a_n, b_n, a_mean = 5, b_mean = 8, a_std = 1, b_std = 1, weight = 1){\n  map(1:sample_n, ~ {\n    # Create samples for conditions a and b\n    a &lt;- tibble(value = rnorm(a_n, a_mean, a_std), condition = 0)\n    b &lt;- tibble(value = rnorm(b_n, b_mean, b_std), condition = 1)\n    df_ab &lt;- bind_rows(a, b)\n\n    # Adjust weights if the argument is given and not 1 \n    if (weight != 1){\n      df_ab &lt;- df_ab %&gt;%\n        mutate(weights = ifelse(condition == 0, weight, 1))\n    }\n\n    return(df_ab)\n  }) %&gt;%\n  imap(~ {\n    if (weight != 1) {\n      # Fit model with weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x, weights = .x$weights)\n    } else {\n      # Fit model without weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x)\n    }\n    # Tidy and return the model coefficients with iteration number\n    broom::tidy(model) %&gt;%\n      select(term, estimate) %&gt;%\n      mutate(iteration = .y)\n  }) %&gt;%\n  bind_rows() %&gt;%\n  pivot_wider(names_from = term, values_from = estimate) %&gt;%\n  rename(alpha = `(Intercept)`, beta = value)\n}\n\n\n\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nbalanced_estimates &lt;- logistic_sim(400, a_n = 200, b_n = 200) %&gt;%\n  mutate(model = \"balanced\")\nunbalanced_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333) %&gt;%\n  mutate(model = \"unbalanced\")\nweighted_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333, weight = 4) %&gt;%\n  mutate(model = \"weighted\")\n\n\n\n\nAs we can see, the unbalanced model underestimates the first group compared to the balanced model. It seems that the intercept is particularly affected, while the slope is relatively similar. However, all the estimated probabilities are affected.\nAssuming that our balanced model is the closest estimate we can get of our population, the unbalanced model that used weights seems to be better fitted. Assuming that our two events are generally equally likely to occur in our population, but our sample is biased, we might be interested in adjusting these estimates using frequency weights.\n\nbind_rows(balanced_estimates, unbalanced_estimates, weighted_estimates) %&gt;%\ngroup_by(model) %&gt;%\nsummarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  mutate(x = list(seq(3,9,.1))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x)) %&gt;%\n  ggplot(aes(x = x, y = y, col = model))+\n  geom_line()+\n  xlab(\"value\")+\n  ylab(\"probability\")+\n  blog_theme()"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "href": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Relationship between odds and weights",
    "text": "Relationship between odds and weights\nBut how do we determine an appropriate frequency weight?\nTo explore this, we experiment with all possible combinations of odds and weights from 1 to 20 in our fictional population. Again, we use the logistic_sim function to estimate our median parameters under each combination.\n\nsample_n = 400\nodds = 1:20\na_n = round(sample_n / (odds + 1))\nb_n = 400 - a_n\nweight = list(1:20)\n\nsim_models &lt;- tibble(sample_n, odds, a_n, b_n, weight) %&gt;%\n  unnest(weight)\n\npsych::headTail(sim_models)\n\n  sample_n odds a_n b_n weight\n1      400    1 200 200      1\n2      400    1 200 200      2\n3      400    1 200 200      3\n4      400    1 200 200      4\n5      ...  ... ... ...    ...\n6      400   20  19 381     17\n7      400   20  19 381     18\n8      400   20  19 381     19\n9      400   20  19 381     20\n\n\n```{r}\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nmodel_estimates &lt;- sim_models %&gt;%\n  mutate(estimates = pmap(list(sample_n = sample_n, a_n = a_n, b_n = b_n, weight = weight), logistic_sim, .progress = TRUE))\n\n```"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#evaluating-parameter-estimates-for-different-odds-and-weights",
    "href": "posts/weights/logistic_sim.html#evaluating-parameter-estimates-for-different-odds-and-weights",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Evaluating parameter estimates for different odds and weights",
    "text": "Evaluating parameter estimates for different odds and weights\nWe can evaluate the performance of each model by looking at the median error at different values of our variable of interest - the mean of the two groups and the midpoint between these means. As you can see in the graph, at the critical value of 6.5, the median error tends to decrease up to a certain weight threshold, after which it increases again. So even in such a simple example, weights only help to a certain extent and must be used with caution.\n\n\nCode\npred &lt;- model_estimates %&gt;%\n  unnest(estimates) %&gt;%\n  group_by(odds, a_n, b_n, weight) %&gt;%\n  summarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  ungroup() %&gt;%\n  mutate(x = list(c(5, 6.5, 8))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x))\n\nbench &lt;- pred %&gt;%\n  filter(weight == 1, odds == 1) %&gt;%\n  select(x, y_bench = y)\n\npred_bench &lt;- pred %&gt;%\n  left_join(bench, by = join_by(x)) %&gt;%\n  mutate(median_error = abs(y_bench - y)) \n\npred_min &lt;- pred_bench %&gt;%\n  filter(x == 6.5) %&gt;%\n  group_by(odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(odds, weight)\n\npred_bench %&gt;%\n  # mutate(odds = as.character(paste0(\"1:\",odds))) %&gt;%\n  ggplot(aes(x = weight, y = median_error, col = factor(x)))+\n  geom_line()+\n  facet_wrap(~odds)+\n  blog_theme()+\n  geom_vline(data = pred_min, aes(xintercept = weight), size = 1, color = \"gray\", linetype= \"dotted\")+\n  ylab(\"median error\")+\n  labs(col = \"value\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#choosing-weights",
    "href": "posts/weights/logistic_sim.html#choosing-weights",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Choosing weights",
    "text": "Choosing weights\nThe threshold with the most balanced estimates seems to be closely related to the odds ratio. Thus, choosing the assumed imbalance of the odds as the weight may be the most appropriate, especially for smaller imbalances. At least in our simple setup. However, deviations for higher imbalances may also be due to the uncertainty of the estimates as the sample size for the minority group decreases. As the odds increase, almost any weight seems to produce more appropriate estimates than the unbalanced model where we did not adjust for sampling bias.\n\nno_weight_bench &lt;- pred_bench %&gt;%\n  filter(weight == 1) %&gt;%\n  select(odds, x, median_error_nw = median_error)\n\npred_nh &lt;- pred_bench %&gt;%\n  left_join(no_weight_bench, by = join_by(odds, x), relationship = \"many-to-many\") %&gt;%\n  mutate(improvement = ifelse(median_error &lt;= median_error_nw, TRUE, FALSE))\n\npred_min_error &lt;- pred_bench %&gt;%\n  group_by(x, odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(x, odds, weight) %&gt;%\n  mutate(best_weight = weight) %&gt;%\n  ungroup()\n\npred_nh %&gt;%\n  left_join(pred_min_error, by = join_by(odds, weight, x)) %&gt;%\n  ggplot(aes(x = weight, y = odds))+\n  geom_tile(aes(fill = improvement ), alpha = .5)+\n  geom_line(aes(x = odds, y = odds), linetype= \"dashed\")+\n  geom_point(aes(x = best_weight, y = odds))+\n  facet_wrap(~factor(x))+\n  blog_theme()+\n  labs(fill = \"improvement to\\nunbalanced model\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#conclusion",
    "href": "posts/weights/logistic_sim.html#conclusion",
    "title": "Exploring frequency weights in logistic regression",
    "section": "Conclusion",
    "text": "Conclusion\nDepending on the scenario, weights may be helpful to deal with sampling bias and class imbalance that does not reflect the population (at least in a simple setup, like the one we worked with). However, they should be used with prior knowledge of the population to not overadjust the sampling bias. One solution might be to adjust the observed odds by the expected odds ratio in the population, in the group affected by the sampling bias. The natural class imbalance should also be maintained in the logistic regression unless you have good reasons not to."
  },
  {
    "objectID": "posts/network/network.html",
    "href": "posts/network/network.html",
    "title": "Correlation Networks in R",
    "section": "",
    "text": "To create a correlation network, we will use the BFI personality data from the {psych} package. First we will import all the necessary packages and set the font for our plot. We can import these from our system using extrafont. In the next step we will do a quick clean up of our data to include only the columns we want to visualise.\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(corrr)\nlibrary(extrafont) # font_import() to import your system fonts\n\nfont = \"Inclusive Sans\"\n\ndf &lt;- psych::bfi %&gt;%\n  tibble() %&gt;%\n  select(-c(age, education, gender))\n\nThe correlate() function from the {corrr} package allows us to define a specific correlation. However, we will use the Pearson correlation as the default setting. Next, we will switch to a long format using stretch() and only keep correlations higher than .2 or lower than -.2. This is something to play around with. Depending on how many variables you have in your data, your plot could get really messy if you include all the possible relationships between your variables.\n\ngraph_data &lt;- df %&gt;% \n    corrr::correlate() %&gt;% \n   corrr::stretch() %&gt;% \n    filter(abs(r) &gt; .2)\n\nThe last step is to build our visualisation. The font variable is the one we set above. In comparison to out of the box tools, this approach with {ggraph} and {igraph} allows us to adjust all the settings you are interested in, such as the size or colour of the nodes. We might also choose a different color scaling, for example if we only include postive correllations. In this specific example, I changed the background color to transparent.\n\ngraph_data %&gt;%\n    graph_from_data_frame(directed = FALSE) %&gt;%\n   ggraph(layout = \"kk\") +\n    geom_edge_link(aes(color = r, alpha = r), edge_width = 1) +\n    guides(edge_alpha = \"none\") +\n    scale_edge_colour_gradientn(limits = c(-1, 1), colors = c(\"firebrick2\", \"white\", \"dodgerblue2\")) +\n    geom_node_point(color = \"black\", size = 2) +\n    geom_node_text(aes(label = name), family = font, repel = TRUE) +\n    theme_graph(base_family = font, title_size = 10) +\n    theme(\n      plot.background = element_rect(fill = \"transparent\", colour = NA),\n        text = element_text(family = font),\n        legend.title = element_text(family = font),\n        legend.text = element_text(family = font)\n    )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "probably wrong",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIntroduction to partial residual plots\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\n\n\nPartial residual plots can tell you how you should adjust your model and even if have you have missed something. They are especially useful to spot interactions in your data. In this post we will look at some handy ways you can them and create PSP with tidy tools\n\n\n\n\n\n\nJun 15, 2024\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation Networks in R\n\n\n\n\n\n\n\nR\n\n\nVisualisation\n\n\nTutorial\n\n\n\n\nPlotting correlations as networks can give you a good first impression of the interconnectivity of your variables. This is a quick tutorial on how to create correlation networks in R.\n\n\n\n\n\n\nJun 12, 2024\n\n\n\n\n\n\n  \n\n\n\n\nExploring frequency weights in logistic regression\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\n\n\nFrequency weights can be used in logistic regression to address class imbalance caused by sampling bias. In this post, we will use simulations to explore how to effectively choose weights and visually understand the benefits of this approach.\n\n\n\n\n\n\nJun 4, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, Iâm Hendrik. Currently in my masterâs studies in primary education, I also work as a Research Assistant in Clinical Child and Adolescent Psychology and Psychotherapy at the University of Wuppertal. This blog is my space to share everything that fascinates me about R and quantitative research methods"
  }
]