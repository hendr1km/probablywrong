[
  {
    "objectID": "posts/weights/logistic_sim.html",
    "href": "posts/weights/logistic_sim.html",
    "title": "Exploring weights in logistic regression",
    "section": "",
    "text": "In Logistic Regression, we estimate a linear model for the log odds of observing an event across different levels of our predictor. These log odds can be transformed into probabilities, which then create the characteristic S-curve.\nAt the core, it all boils down to odds. Having odds of 1:1 in our underlying data generating process means both events are generally equally likely to occur, each with a probability of 50%. Our fictional population for the simulation follows this setup, where the probabilities of our events are dependent on values drawn from normal distributions for both groups, each with a mean of 5 and 8 and a standard deviation of 1.\nlibrary(tidyverse)\n\n    a &lt;- tibble(value = rnorm(4000, 5, 1), condition = \"a\")\n    b &lt;- tibble(value = rnorm(4000, 8, 1), condition = \"b\")\n    df_ab &lt;- bind_rows(a, b)\n\ndf_ab %&gt;%\n  ggplot(aes(x = value, y = condition, fill = condition))+\n  ggdist::stat_halfeye()+ \n  scale_x_continuous(breaks = seq(1,12,1))+\n  blog_theme()+\n  guides(fill = \"none\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#simulating-imbalance",
    "href": "posts/weights/logistic_sim.html#simulating-imbalance",
    "title": "Exploring weights in logistic regression",
    "section": "Simulating imbalance",
    "text": "Simulating imbalance\nTo get an idea on how sample weights effect the logistic regression outcomes, we generate some simulations from our fictional population under different conditions. Each sample will consist of 400 samples with N = 400 drawing from our population.\n\nBalanced Dataset: Each sample will have a 1:1 ratio of our two classes.\nUnbalanced Dataset: Each sample will have a 1:4 ratio, representing a class imbalance.\nWeighted Unbalanced Dataset: Similar to the unbalanced dataset, but here each sample in the minority class will be assigned a weight of 4 to counteract the imbalance.\n\nFor each sample under these conditions, we will perform a logistic regression using the logistic_sim function on each sample.\nAfterwards we will compute the median of the estimated parameters (e.g., coefficients and intercepts) from each condition to get a robust measure of central tendency, less affected by outliers than the mean.\nFinally, we will aggregate these median parameter estimates across all conditions into a single logistic regression model for each condition.\n\nlogistic_sim functionconditions\n\n\n\nlogistic_sim &lt;- function(sample_n, a_n, b_n, a_mean = 5, b_mean = 8, a_std = 1, b_std = 1, weight = 1){\n  map(1:sample_n, ~ {\n    # Create samples for conditions a and b\n    a &lt;- tibble(value = rnorm(a_n, a_mean, a_std), condition = 0)\n    b &lt;- tibble(value = rnorm(b_n, b_mean, b_std), condition = 1)\n    df_ab &lt;- bind_rows(a, b)\n\n    # Adjust weights if the argument is given and not 1 \n    if (weight != 1){\n      df_ab &lt;- df_ab %&gt;%\n        mutate(weights = ifelse(condition == 0, weight, 1))\n    }\n\n    return(df_ab)\n  }) %&gt;%\n  imap(~ {\n    if (weight != 1) {\n      # Fit model with weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x, weights = .x$weights)\n    } else {\n      # Fit model without weights\n      model &lt;- glm(condition ~ value, family = binomial, data = .x)\n    }\n    # Tidy and return the model coefficients with iteration number\n    broom::tidy(model) %&gt;%\n      select(term, estimate) %&gt;%\n      mutate(iteration = .y)\n  }) %&gt;%\n  bind_rows() %&gt;%\n  pivot_wider(names_from = term, values_from = estimate) %&gt;%\n  rename(alpha = `(Intercept)`, beta = value)\n}\n\n\n\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nbalanced_estimates &lt;- logistic_sim(400, a_n = 200, b_n = 200) %&gt;%\n  mutate(model = \"balanced\")\nunbalanced_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333) %&gt;%\n  mutate(model = \"unbalanced\")\nweighted_estimates &lt;- logistic_sim(400, a_n = 67, b_n = 333, weight = 4) %&gt;%\n  mutate(model = \"weighted\")\n\n\n\n\nAs we can see, the estimates from the unbalanced model are quite off compared to those from the balanced one. It appears that the intercept is especially affected, while the slope seems relatively similar, though all estimated probabilities are nonetheless influenced.\nThe unbalanced model that used weights does not seem too much off. Assuming that our two events are equally likely in our population, but our sample is biased, choosing the right weight might be a solution.\n\nbind_rows(balanced_estimates, unbalanced_estimates, weighted_estimates) %&gt;%\ngroup_by(model) %&gt;%\nsummarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  mutate(x = list(seq(3,9,.1))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x)) %&gt;%\n  ggplot(aes(x = x, y = y, col = model))+\n  geom_line()+\n  xlab(\"value\")+\n  ylab(\"probability\")+\n  blog_theme()"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "href": "posts/weights/logistic_sim.html#relationship-between-odds-and-weights",
    "title": "Exploring weights in logistic regression",
    "section": "Relationship between odds and weights",
    "text": "Relationship between odds and weights\nBut how do we determine the appropriate weight to choose?\nTo explore this, we experiment with all possible combinations of odds and weights ranging from 1 to 20 in our fictional population. We employ the logistic_sim function again to estimate our median parameters under each combination.\nThis approach allows us to systematically assess how different weights influence the logistic regression model’s ability to handle varying levels of class imbalance.\n\nsample_n = 400\nodds = 1:20\na_n = round(sample_n / (odds + 1))\nb_n = 400 - a_n\nweight = list(1:20)\n\nsim_models &lt;- tibble(sample_n, odds, a_n, b_n, weight) %&gt;%\n  unnest(weight)\n\npsych::headTail(sim_models)\n\n  sample_n odds a_n b_n weight\n1      400    1 200 200      1\n2      400    1 200 200      2\n3      400    1 200 200      3\n4      400    1 200 200      4\n5      ...  ... ... ...    ...\n6      400   20  19 381     17\n7      400   20  19 381     18\n8      400   20  19 381     19\n9      400   20  19 381     20\n\n\n```{r}\n\ndoParallel::registerDoParallel(cores = parallel::detectCores())\n\nmodel_estimates &lt;- sim_models %&gt;%\n  mutate(estimates = pmap(list(sample_n = sample_n, a_n = a_n, b_n = b_n, weight = weight), logistic_sim, .progress = TRUE))\n\n```"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#evaluating-parameter-estimates-for-different-odds-and-weights",
    "href": "posts/weights/logistic_sim.html#evaluating-parameter-estimates-for-different-odds-and-weights",
    "title": "Exploring weights in logistic regression",
    "section": "Evaluating parameter estimates for different odds and weights",
    "text": "Evaluating parameter estimates for different odds and weights\nWe assess the performance of each model by examining the median error at different values of our variable of interest—the mean of both groups and the midpoint value between these means. Our findings indicate that the median error tends to decrease up to a certain weight threshold, after which it tends to increase again.\n\n#| code-fold: true\n\npred &lt;- model_estimates %&gt;%\n  unnest(estimates) %&gt;%\n  group_by(odds, a_n, b_n, weight) %&gt;%\n  summarise(alpha.median = median(alpha),\n            beta.median = median(beta)) %&gt;%\n  ungroup() %&gt;%\n  mutate(x = list(c(5, 6.5, 8))) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = boot::inv.logit(alpha.median + beta.median * x))\n\nbench &lt;- pred %&gt;%\n  filter(weight == 1, odds == 1) %&gt;%\n  select(x, y_bench = y)\n\npred_bench &lt;- pred %&gt;%\n  left_join(bench, by = join_by(x)) %&gt;%\n  mutate(median_error = abs(y_bench - y)) \n\npred_min &lt;- pred_bench %&gt;%\n  filter(x == 6.5) %&gt;%\n  group_by(odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(odds, weight)\n\npred_bench %&gt;%\n  # mutate(odds = as.character(paste0(\"1:\",odds))) %&gt;%\n  ggplot(aes(x = weight, y = median_error, col = factor(x)))+\n  geom_line()+\n  facet_wrap(~odds)+\n  blog_theme()+\n  geom_vline(data = pred_min, aes(xintercept = weight), size = 1, color = \"gray\", linetype= \"dotted\")+\n  ylab(\"median error\")+\n  labs(col = \"value\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#choosing-weights",
    "href": "posts/weights/logistic_sim.html#choosing-weights",
    "title": "Exploring weights in logistic regression",
    "section": "Choosing weights",
    "text": "Choosing weights\nThis threshold seems to be closely related to the odds ratio. Thus, choosing the assumed imbalance of the odds as the weight is particularly beneficial, especially for lower imbalances. However, deviations at higher imbalances may also be due to the uncertainty of estimates as the sample size for the minority group decreases. As the odds increase, the need for precise weight selection becomes less critical compared to using the unbalanced model.\nFor lower odds, there’s a tolerance for selecting an incorrect weight. For example, in our specific simulation, at odds of 1:2, the weight estimate could vary from 2 weights off to 19 off, depending on the predictor value, and still provide more precise estimates than the unweighted model. However, while there seems to be some tolerance, it’s important to note that no broad generalizations should be drawn from our simplified simulation.\n\nno_weight_bench &lt;- pred_bench %&gt;%\n  filter(weight == 1) %&gt;%\n  select(odds, x, median_error_nw = median_error)\n\npred_nh &lt;- pred_bench %&gt;%\n  left_join(no_weight_bench, by = join_by(odds, x), relationship = \"many-to-many\") %&gt;%\n  mutate(improvement = ifelse(median_error &lt;= median_error_nw, TRUE, FALSE))\n\npred_min_error &lt;- pred_bench %&gt;%\n  group_by(x, odds) %&gt;%\n  filter(median_error == min(median_error)) %&gt;%\n  select(x, odds, weight) %&gt;%\n  mutate(best_weight = weight) %&gt;%\n  ungroup()\n\npred_nh %&gt;%\n  left_join(pred_min_error, by = join_by(odds, weight, x)) %&gt;%\n  ggplot(aes(x = weight, y = odds))+\n  geom_tile(aes(fill = improvement ), alpha = .5)+\n  geom_line(aes(x = odds, y = odds), linetype= \"dashed\")+\n  geom_point(aes(x = best_weight, y = odds))+\n  facet_wrap(~factor(x))+\n  blog_theme()+\n  labs(fill = \"improvement to\\nunbalanced model\")"
  },
  {
    "objectID": "posts/weights/logistic_sim.html#conclusion",
    "href": "posts/weights/logistic_sim.html#conclusion",
    "title": "Exploring weights in logistic regression",
    "section": "Conclusion",
    "text": "Conclusion\nWeights are a practical approach to address data imbalances that don’t reflect the actual odds in the population. But, it’s important to adjust these weights carefully, based on what we know about the population to get better results. Also, it’s good to recognize that natural class imbalances in your data should be seen as normal features, not issues to be fixed.\nComparing different models, with and without weight adjustments, especially in terms of their predictive accuracy, can be quite helpful. Just keep in mind, our simple simulation doesn’t cover all the bases. So, when you’re doing your research, make sure to take into account the specific factors relevant to your field, as the literature suggests."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "probably wrong",
    "section": "",
    "text": "Exploring weights in logistic regression\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\n\n\nWeights can be used in logistic regression to deal with class imbalance. In this post, we use simulations to estimate the costs and benefits of this approach.\n\n\n\n\n\n\nMay 26, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, I’m Hendrik. Currently in my master’s studies in primary education, I also work as a Research Assistant in Clinical Child and Adolescent Psychology and Psychotherapy at the University of Wuppertal. This blog is my space to share everything that fascinates me about R and quantitative research methods"
  }
]